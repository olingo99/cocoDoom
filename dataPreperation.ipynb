{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning - CocoDoom\n",
    "## Introduction\n",
    "Dans ce rapport nous allons vous présenter notre travail de deep learning sur le dataset CocoDoom. Ce dataset est composé d'images extraites de 3 parties de Doom, divisés chacunes en 32 maps. Les images sont des captures d'écran du jeu, et sont labellisées avec les objets présents dans l'image (bounding box et catégorie). Le but de ce projet est de créer un modèle capable de prédire les objets présents dans ces images. \n",
    "\n",
    "Nous avons décidé d'utiliser yolo pour ce projet car il permet de faire de la classification ainsi que de la segmentation d'images.\n",
    "\n",
    "## Préparation des données\n",
    "La première étape fût de préparer les données extraites du dataset, celles ci étant au format MS Coco ( Bounding box = [x_min, y_min, width, height]), Nous avons donc dû les convertir au format Yolo (Bounding box = [x_center, y_center, width, height]). Attention, au format yolo les données sont normalisées par rapport à la taille de l'image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11, 38971), (38, 13403), (12, 12770), (14, 11754), (2, 11524), (18, 7090), (10, 6893), (1, 6760), (37, 5912), (77, 5338), (34, 5212), (45, 4764), (54, 4686), (17, 4243), (20, 4212), (8, 3898), (33, 3870), (73, 3768), (5, 3738), (53, 3549), (93, 2865), (46, 2666), (23, 2639), (30, 2543), (63, 2424), (42, 2272), (31, 2262), (92, 1932), (66, 1764), (91, 1728), (22, 1708), (64, 1637), (70, 1476), (15, 1464), (32, 1374), (65, 1350), (69, 1302), (39, 1140), (35, 1082), (67, 970), (106, 887), (9, 801), (59, 723), (3, 721), (82, 656), (95, 650), (57, 650), (126, 594), (97, 593), (55, 586), (19, 548), (68, 541), (36, 541), (6, 535), (127, 534), (75, 469), (16, 450), (43, 448), (21, 443), (96, 410), (90, 388), (71, 369), (121, 367), (83, 366), (56, 339), (76, 315), (89, 300), (62, 297), (124, 292), (58, 286), (7, 283), (78, 273), (100, 265), (109, 263), (88, 240), (72, 197), (94, 178), (99, 177), (110, 176), (44, 175), (118, 155), (108, 151), (84, 141), (74, 140), (80, 140), (60, 129), (134, 127), (117, 126), (47, 125), (123, 119), (125, 114), (4, 113), (79, 104), (119, 102)]\n",
      "[(11, 7545), (38, 2912), (12, 2448), (2, 2325), (14, 1944), (18, 1500), (37, 1489), (10, 1359), (1, 1339), (77, 1082), (34, 1072), (33, 1044), (45, 937), (8, 811), (17, 808), (5, 757), (54, 755), (20, 654), (42, 653), (30, 642), (73, 614), (53, 588), (23, 551), (31, 514), (93, 512), (46, 497), (63, 473), (22, 401), (15, 396), (66, 346), (92, 303), (35, 301), (69, 256), (91, 255), (39, 247), (70, 226), (64, 225), (65, 220), (67, 204), (32, 201), (9, 162), (106, 154), (3, 146), (59, 132), (55, 129), (83, 120), (82, 120), (57, 114), (95, 104), (126, 101), (6, 90), (68, 89), (121, 89), (97, 87), (36, 83), (127, 83), (43, 79), (21, 79), (75, 72), (90, 70), (16, 69), (56, 66), (71, 64), (58, 63), (96, 60), (19, 59), (76, 53), (124, 48), (99, 44), (62, 39), (108, 38), (44, 37), (94, 37), (78, 36), (72, 36), (47, 35), (4, 34), (100, 33), (110, 31), (109, 31), (88, 30), (7, 29), (74, 27), (60, 27), (89, 25), (80, 21), (79, 21), (123, 19), (84, 18), (134, 17), (117, 17), (125, 14), (119, 14), (118, 12)]\n",
      "[(11, 4414), (2, 1439), (12, 1397), (38, 1220), (14, 1011), (34, 933), (10, 864), (1, 818), (33, 765), (18, 627), (77, 621), (42, 511), (45, 508), (20, 466), (8, 457), (54, 436), (37, 420), (17, 410), (5, 400), (73, 378), (23, 376), (30, 329), (53, 328), (15, 323), (46, 321), (93, 285), (31, 268), (63, 245), (22, 244), (92, 207), (66, 200), (39, 189), (35, 178), (64, 160), (70, 149), (91, 142), (67, 137), (65, 123), (69, 121), (36, 119), (32, 101), (9, 101), (59, 98), (97, 73), (68, 70), (3, 65), (57, 63), (16, 58), (55, 56), (82, 54), (126, 49), (106, 49), (96, 45), (6, 43), (90, 43), (95, 41), (71, 41), (19, 39), (62, 39), (21, 37), (76, 36), (99, 36), (83, 36), (56, 35), (127, 34), (121, 33), (75, 31), (58, 29), (43, 29), (78, 27), (100, 27), (72, 25), (4, 23), (88, 22), (44, 21), (89, 21), (74, 20), (47, 19), (79, 18), (80, 17), (124, 17), (109, 16), (94, 15), (134, 14), (60, 14), (117, 14), (84, 14), (119, 13), (7, 12), (118, 11), (110, 10), (123, 10), (125, 9), (108, 8)]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os \n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "createdata = False\n",
    "\n",
    "src = \"cocodoomData/\"\n",
    "width = 320\n",
    "height = 200\n",
    "\n",
    "names = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 46, 47, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 106, 108, 109, 110, 117, 118, 119, 121, 123, 124, 125, 126, 127, 134]\n",
    "\n",
    "for fname in [\"run-train.json\",\"run-val.json\", \"run-test.json\"]:\n",
    "    f = json.load(open(\"cocodoomData/\"+fname,\"r\"))\n",
    "    idImagesLink = {}\n",
    "    for images in f[\"images\"]:\n",
    "        idImagesLink[images[\"id\"]] = {\"file_name\":images[\"file_name\"], \"bbox\":[], \"category_id\":[]}\n",
    "\n",
    "    for annotation in f[\"annotations\"]:\n",
    "        idImagesLink[annotation[\"image_id\"]][\"bbox\"].append(annotation[\"bbox\"])\n",
    "        idImagesLink[annotation[\"image_id\"]][\"category_id\"].append(annotation[\"category_id\"])\n",
    "    count = defaultdict(int)\n",
    "    for elem in idImagesLink.values():\n",
    "        for cat in elem[\"category_id\"]:\n",
    "            count[cat] += 1\n",
    "\n",
    "    print(sorted(count.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    if (createdata):\n",
    "        for elem in idImagesLink:              \n",
    "            dest = \"data/\"+idImagesLink[elem][\"file_name\"].replace(\"/rgb\",\"images\")\n",
    "            dest = re.sub(r'\\bmap\\d{1,2}', '', dest)\n",
    "            dest = dest.split('/')\n",
    "            dest[1], dest[2] = dest[2], dest[1]\n",
    "            dest = '/'.join(dest)\n",
    "            print(dest)\n",
    "            shutil.copy(src+idImagesLink[elem][\"file_name\"], dest)\n",
    "            with open(dest.replace(\"images\",\"labels\").replace(\".png\",\".txt\"),\"w\") as f:\n",
    "                # if len(idImagesLink[elem][\"bbox\"]) == 0:\n",
    "                #     f.write(str(0)+\"\\n\")\n",
    "                for i, bbox in enumerate(idImagesLink[elem][\"bbox\"]):\n",
    "                    xmin = bbox[0]\n",
    "                    ymin = bbox[1]\n",
    "                    xmax = bbox[0]+bbox[2]\n",
    "                    ymax = bbox[1]+bbox[3]\n",
    "                    center_x = min(((xmin+xmax)//2)/width,1.0)\n",
    "                    center_y = min(((ymin+ymax)//2)/height,1.0)\n",
    "                    widthbb = min((xmax-xmin)/width,1)\n",
    "                    heightbb = min((ymax-ymin)/height,1)\n",
    "                    f.write(str(names.index(idImagesLink[elem][\"category_id\"][i]))+\" \"+str(center_x)+\" \"+str(center_y)+\" \"+str(widthbb)+\" \"+str(heightbb)+\"\\n\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut observer que le dataset est fortment désequilibrer ce qui peut dégrader les resultat de notre modèle, cependant rééquilibgrer le dataset est une tache délicate. Deux option s'offre a nous: l'undersampling et l'oversampling. Dans notre cas, il est compliqué de faire de l'undersampling car les images contenant les classe majoritaire conteinnet sans doute aussi des objets d'autre classe. L'oversampling des classe minoritaire est tout aussi compliqué pour la même raison. Nous avons donc décidé de ne pas rééquilibrer le dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Nous devons crée un fichier de config pour l'entrainement du model. Ce fichier contient les informations suivantes :\n",
    "- path: path vers les images d'entrainement\n",
    "- train: path vers le fichier contenant les images d'entrainement\n",
    "- valid: path vers le fichier contenant les images de validation\n",
    "- names: Ids et noms des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "data = dict(\n",
    "path= '/Users/engel/Documents/cocoDoom/datatest',\n",
    "train='images/run1',\n",
    "val= 'images/run1',\n",
    "\n",
    "names = \n",
    "{\n",
    "0 :  \"a\" ,\n",
    "1 :  \"b\" ,\n",
    "2 :  \"c\" ,\n",
    "3 :  \"d\" ,\n",
    "4 :  \"e\" ,\n",
    "5 :  \"f\" ,\n",
    "6 :  \"g\" ,\n",
    "7 :  \"h\" ,\n",
    "8 :  \"i\" ,\n",
    "9 :  \"j\" ,\n",
    "10 :  \"k\" ,\n",
    "11 :  \"l\" ,\n",
    "12 :  \"m\" ,\n",
    "13 :  \"o\" ,\n",
    "14 :  \"p\" ,\n",
    "15 :  \"q\" ,\n",
    "16 :  \"r\" ,\n",
    "17 :  \"s\" ,\n",
    "18 :  \"t\" ,\n",
    "19 :  \"u\" ,\n",
    "20 :  \"v\" ,\n",
    "21 :  \"w\" ,\n",
    "22 :  \"x\" ,\n",
    "23 :  \"z\" ,\n",
    "24 :  \"A\" ,\n",
    "25 :  \"B\" ,\n",
    "26 :  \"C\" ,\n",
    "27 :  \"D\" ,\n",
    "28 :  \"E\" ,\n",
    "29 :  \"F\" ,\n",
    "30 :  \"G\" ,\n",
    "31 :  \"H\" ,\n",
    "32 :  \"I\" ,\n",
    "33 :  \"J\" ,\n",
    "34 :  \"K\" ,\n",
    "35 :  \"L\" ,\n",
    "36 :  \"M\" ,\n",
    "37 :  \"O\" ,\n",
    "38 :  \"P\" ,\n",
    "39 :  \"Q\" ,\n",
    "40 :  \"R\" ,\n",
    "41 :  \"S\" ,\n",
    "42 :  \"T\" ,\n",
    "43 :  \"U\" ,\n",
    "44 :  \"V\" ,\n",
    "45 :  \"W\" ,\n",
    "46 :  \"X\" ,\n",
    "47 :  \"Z\" ,\n",
    "48 :  \"aa\" ,\n",
    "49 :  \"ba\" ,\n",
    "50 :  \"ca\" ,\n",
    "51 :  \"da\" ,\n",
    "52 :  \"ea\" ,\n",
    "53 :  \"fa\" ,\n",
    "54 :  \"ga\" ,\n",
    "55 :  \"ha\" ,\n",
    "56 :  \"ia\" ,\n",
    "57 :  \"ja\" ,\n",
    "58 :  \"ka\" ,\n",
    "59 :  \"la\" ,\n",
    "60 :  \"ma\" ,\n",
    "61 :  \"oa\" ,\n",
    "62 :  \"pa\" ,\n",
    "63 :  \"qa\" ,\n",
    "64 :  \"ra\" ,\n",
    "65 :  \"sa\" ,\n",
    "66 :  \"ta\" ,\n",
    "67 :  \"ua\" ,\n",
    "68 :  \"va\" ,\n",
    "69 :  \"wa\" ,\n",
    "70 :  \"xa\" ,\n",
    "71 :  \"za\" ,\n",
    "72 :  \"Aa\" ,\n",
    "73 :  \"Ba\" ,\n",
    "74 :  \"Ca\" ,\n",
    "75 :  \"Da\" ,\n",
    "76 :  \"Ea\" ,\n",
    "77 :  \"Fa\" ,\n",
    "78 :  \"Ga\" ,\n",
    "79 :  \"Ha\" ,\n",
    "80 :  \"Ia\" ,\n",
    "81 :  \"Ja\" ,\n",
    "82 :  \"Ka\" ,\n",
    "83 :  \"La\" ,\n",
    "84 :  \"Ma\" ,\n",
    "85 :  \"Oa\" ,\n",
    "86 :  \"Pa\" ,\n",
    "87 :  \"Qa\" ,\n",
    "88 :  \"Ra\" ,\n",
    "89 :  \"Sa\" ,\n",
    "90 :  \"Ta\" ,\n",
    "91 :  \"Ua\" ,\n",
    "92 :  \"Va\" ,\n",
    "93 :  \"Wa\" ,\n",
    "\n",
    "}\n",
    ")\n",
    "\n",
    "with open('data2.yml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle\n",
    "Nous pouvons désormais entrainer notre modèle sur notre dataset, nous utilison un modèle préetnainé sur le dataset 'COCO' pour la detection/segmentation et sur el dataset 'Imagenet' pour la classification. Nous avons entrainé le modèle sur 15 epochs car au delà on depasse les douzes heures maximus de run de kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb disabled\n",
    "!yolo detect train data='config.yaml' model='yolov8n.pt' epochs=15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédictions\n",
    "prediction sur des nouvelle donnée qu'on dump en json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "model = YOLO('results/runs/detect/train/weights/best.pt')  # initialize\n",
    "result = model.predict('data/images/run3/', conf=0.75)  # predict\n",
    "\n",
    "r = {}\n",
    "for i in result:\n",
    "    r[i.path] = (i.tojson())\n",
    "json.dump(r, open(\"resultconf0.75.json\",\"w\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Nous pouvons désormais évaluer les performances de notre modèle. Pour ce faire nous devons charger les données prédites ainsi que les données réeles afin de pouvoir les comparer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = json.load(open(\"resultconf0.75.json\",\"r\"))\n",
    "predictedData = {}\n",
    "for key, elem in Data.items():\n",
    "    print(key, elem)\n",
    "    predictedData[key] = json.loads(elem)\n",
    "\n",
    "f = json.load(open(\"cocodoomData/run-test.json\",\"r\"))\n",
    "groundTruthData = {}\n",
    "for images in f[\"images\"]:\n",
    "    groundTruthData[images[\"id\"]] = {\"file_name\":images[\"file_name\"], \"bbox\":[], \"category_id\":[]}\n",
    "\n",
    "for annotation in f[\"annotations\"]:\n",
    "    box = annotation[\"bbox\"]\n",
    "    box[2] += box[0]\n",
    "    box[3] += box[1]\n",
    "    groundTruthData[annotation[\"image_id\"]][\"bbox\"].append(box)\n",
    "    groundTruthData[annotation[\"image_id\"]][\"category_id\"].append(annotation[\"category_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIoU(predictedBox, trueBox):\n",
    "    x1 = max(predictedBox[0], trueBox[0])\n",
    "    y1 = max(predictedBox[1], trueBox[1])\n",
    "    x2 = min(predictedBox[2], trueBox[2])\n",
    "    y2 = min(predictedBox[3], trueBox[3])\n",
    "    intersection = max(0, x2-x1) * max(0, y2-y1)\n",
    "    union =( predictedBox[2]-predictedBox[0])*(predictedBox[3]-predictedBox[1]) + (trueBox[2]-trueBox[0])*(trueBox[3]-trueBox[1]) - intersection\n",
    "    # print(\"Intersection: \", intersection)\n",
    "    # print(\"Union: \", union)\n",
    "    return intersection/union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les données chargées, nous pouvons calculer les metrics suivants:\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "calcul des metrics, je vais probablment tout changer demain il y a des trucs préfait par yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = {}\n",
    "iouMoyenne = 0\n",
    "falsePrediction = 0\n",
    "totErrorNbBox = 0\n",
    "total = 0\n",
    "for predicetedkey, predictedvalue,truevalue in zip(predictedData.keys(), predictedData.values(),groundTruthData.values()):\n",
    "    iou[predicetedkey] = {'metric':[]}\n",
    "    ErrorNbBox = 0\n",
    "    if (len(predictedvalue) != len(truevalue[\"bbox\"])):\n",
    "        if (len(predictedvalue) > len(truevalue[\"bbox\"])):\n",
    "            ErrorNbBox = len(predictedvalue) - len(truevalue[\"bbox\"])\n",
    "        else:\n",
    "            ErrorNbBox = len(truevalue[\"bbox\"]) - len(predictedvalue)\n",
    "        totErrorNbBox += abs(ErrorNbBox)\n",
    "    for trueBox,trueid in zip(truevalue[\"bbox\"], truevalue[\"category_id\"]):\n",
    "        computedIoU = 0\n",
    "        predictedIdMaxIoU = 0\n",
    "        for predicted in predictedvalue:\n",
    "            predictedBox = [predicted['box'][\"x1\"], predicted['box'][\"y1\"], predicted['box'][\"x2\"], predicted['box'][\"y2\"]]\n",
    "            predictedId = predicted[\"class\"]+1\n",
    "            newIoU = computeIoU(predictedBox, trueBox)\n",
    "            if (computedIoU<newIoU):\n",
    "                computedIoU = newIoU\n",
    "                predictedIdMaxIoU = predictedId\n",
    "        if (len(predictedvalue) != 0):\n",
    "            falsePrediction += 1 if predictedIdMaxIoU!=trueid else 0\n",
    "            if predictedIdMaxIoU!=trueid:\n",
    "                print(\"Predicted ID: \", predictedId)\n",
    "                print(\"True ID: \", trueid)\n",
    "        total += 1\n",
    "        iouMoyenne += computedIoU\n",
    "        iou[predicetedkey]['metric'].append({ \"iou\": computedIoU, \"predictedId\": predictedId})\n",
    "        iou[predicetedkey]['errorNbBox'] = ErrorNbBox\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "print(\"IoU Moyenne: \", iouMoyenne/total)\n",
    "print(\"False Prediction: \", falsePrediction)\n",
    "print(\"Total: \", total)\n",
    "print(\"Total Error Nb Box: \", totErrorNbBox)\n",
    "iou['globalMetrics'] = {\"IoUMoyenne\": iouMoyenne/total, \"FalsePrediction\": falsePrediction, \"Total\": total, \"TotalErrorNbBox\": totErrorNbBox}\n",
    "json.dump(iou, open(\"iouconf0.75.json\",\"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelstr in ['resultLarge30', 'resultLarge', 'results']:  #dans l'ordre c'est val3, val4 et val 5\n",
    "    model = YOLO(f'{modelstr}/runs/detect/train/weights/best.pt')\n",
    "    res = model.val()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amélioration du modele\n",
    "\n",
    "Les même manipulation on été faites pour 2 autres modèles: le modèleyoloV8l entrainé sur 15 epochs et le modèles yolov8l entrainé sur 30 epochs (en réentrainant le modele a partir des poids de l'entrainement 15 epochs)\n",
    "\n",
    "Le modèle yolov8l (large) est un plus gros que yolov8n (nano). Ce qui le rends plus lent mais plus précis. (cfr https://docs.ultralytics.com/tasks/detect/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultat et interpretation\n",
    "\n",
    "\n",
    "<img src=\"resultLarge30\\runs\\detect\\train\\val_batch1_labels.jpg\" alt= “” width=\"value\" height=300>\n",
    "\n",
    "\n",
    "<img src=\"resultLarge30\\runs\\detect\\train\\val_batch1_pred.jpg\" alt= “” width=\"value\" height=300>\n",
    "\n",
    "## confusions matrix\n",
    "\n",
    "EN analisant la matrice de confusion resultant de l'entrainement du modele on peut observer que l'erreur la plus prédominante est la detection d'un objet comme etant un background. Cela veut dire que le modèle ne détecte rien dans l'image, cette erreur peut etre due a un manque d'entrainement ou a un threshold de confiance trop élevé. L'erreur inverse apparait aussi, c'est a dire que le modèle detecte un background comme etant un objet, cela est sans doute aussi du a un manque d'entrainement ou a un threshold de confiance trop bas. Plusieur classe sont aussi regulierement confonudes, probablment du a une ressemblance entre les objets de ces classes.\n",
    "\n",
    "Les matrices sont similaire pour les 3 modèles.\n",
    "\n",
    "<img src=\"resultLarge30\\runs\\detect\\train\\confusion_matrix_normalized.png\" alt= “” width=\"value\" height=600>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En analysant les courbe fournie par yolo apres l'enrtainement on peu tobserver que la loss diminue au fur et a mesure des epochs, ce qui est normal. Cependant elle n'atteind j'amais de plateau, ce qui inquie que le modèle pourrait encore s'améliorer si on l'entrainait plus longtemps.\n",
    "\n",
    "En comparant les donnée, on peut voir que le modele entrainé sur 30 epochs a une loss de validation plus faible. De plus les metrics map, précision et recall de celui-ci sont aussi plus elevée.(mAP0.5 represente la mean average precission avec un seuil de IoU de 0.5 pour determiner le true positives ).\n",
    "\n",
    "Il y a peu de différence entre les deux modèles entrainés sur 15 epochs, cependant il est probable qu'avec un entrainement plus long, le modèle yolov8l (large) surpasse le modèle yolov8n (nano).\n",
    "\n",
    "Le model entrainé sur 30 epochs est donc le meilleur des 3.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "large30\n",
    "<img src=\"resultLarge30\\runs\\detect\\train\\results.png\" alt= “” width=\"value\" height=600>\n",
    "\n",
    "large\n",
    "<img src=\"resultLarge\\runs\\detect\\train\\results.png\" alt= “” width=\"value\" height=600>\n",
    "\n",
    "nano\n",
    "<img src=\"results\\runs\\detect\\train\\results.png\" alt= “” width=\"value\" height=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre metric interessante est le Iou (intersection over Union) qui compare les bounding box prédite au bounding box réel.\n",
    "\n",
    "pour nos trois models nous avons un IoU moyen de:\n",
    "\n",
    "| Seuil de confiance/model | yolovv8n | yolov8L 15 epochs | yolov8L 30 epochs |\n",
    "|---|---|---|---|\n",
    "| 0.25 | 0.671 | 0.671 | 0.682 |\n",
    "| 0.5 | 0.593 | 0.593 | 0.606 |\n",
    "| 0.75 | 0.450 | 0.450 | 0.459 |\n",
    "\n",
    "a\n",
    "\n",
    "\n",
    "| Seuil de confiance/model | yolovv8n | yolov8L 15 epochs | yolov8L 30 epochs |\n",
    "|---|---|---|---|\n",
    "| 0.25 | +: 1507 -: 3946 | +: 1507 -: 3946 | +: 1589 -: 3780 |\n",
    "| 0.5 | +: 126 -: 8363 | +: 126 -: 8363 | +: 106 -: 8065 |\n",
    "| 0.75 | +: 11 -: 13100 | +: 11 -: 13100 | +: 14 -: 12901 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, l'entrainement du modèle nous fourni 4 courbe qui permettent d'annalysé les performances du modèle en fonction de seuil de confiance afin de fine-tuner le modèle selon nos besoins.\n",
    "\n",
    "<img src=\"resultLarge30\\runs\\detect\\train\\F1_curve.png\" alt= “” width=\"value\" height=600>\n",
    "<img src=\"resultLarge30\\runs\\detect\\train\\P_curve.png\" alt= “” width=\"value\" height=600>\n",
    "<img src=\"resultLarge30\\runs\\detect\\train\\R_curve.png\" alt= “” width=\"value\" height=600>\n",
    "<img src=\"resultLarge30\\runs\\detect\\train\\PR_curve.png\" alt= “” width=\"value\" height=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Nous avons vu que l'un des facteurs limitant la performance de notre modèle est le nombre d'epochs d'entrainement. En effet, nous avons entrainé notre modèle sur 15 epochs car au delà on depasse les douzes heures maximus de run de kaggle. Une manière de contourner ce modèle et de réentrainer notre modèle sur 15 epochs en utilisant les poids du modèle précédent. On pourrais ainsi continuer à entrainer notre modèle sur 15 epochs jusqu'à ce que les performances ne s'améliorent plus.\n",
    "Un autre facteur important est la taille du modèle. En effet, nous avons commencé avec yolo nano avant de passer a yolo large. Nous avons pu constater que le modèle large est plus performant que le modèle nano. Cependant, le modèle large est plus long à entrainer et plus long à prédire. Il serait donc intéressant de trouver un compromis entre la taille du modèle et ses performances.\n",
    "\n",
    "\n",
    "amelioration :\n",
    "- mettre vrai nom de classe\n",
    "- entrainer plus longtemps\n",
    "- entrainer sur plus de données (utiliser les donnée test pour entrainer)\n",
    "- fine tune le seuil de confiance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
