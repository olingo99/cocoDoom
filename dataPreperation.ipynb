{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning - CocoDoom\n",
    "## Introduction\n",
    "Dans ce rapport nous allons vous présenter notre travail de deep learning sur le dataset CocoDoom. Ce dataset est composé d'images extraites de 3 parties de Doom, divisés chacunes en 32 maps. Les images sont des captures d'écran du jeu, et sont labellisées avec les objets présents dans l'image (bounding box et catégorie). Le but de ce projet est de créer un modèle capable de prédire les objets présents dans ces images. \n",
    "\n",
    "Nous avons décidé d'utiliser yolo pour ce projet car il permet de faire de la classification ainsi que de la segmentation d'images.\n",
    "\n",
    "## Préparation des données\n",
    "La première étape fût de préparer les données extraites du dataset, celles ci étant au format MS Coco ( Bounding box = [x_min, y_min, width, height]), Nous avons donc dû les convertir au format Yolo (Bounding box = [x_center, y_center, width, height]). Attention, au format yolo les données sont normalisées par rapport à la taille de l'image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os \n",
    "import json\n",
    "import re\n",
    "\n",
    "src = \"cocodoomData/\"\n",
    "width = 320\n",
    "height = 200\n",
    "\n",
    "names = [i for i in range(1,135)]\n",
    "\n",
    "for fname in [\"run-train.json\",\"run-val.json\", \"run-test.json\"]:\n",
    "    f = json.load(open(\"cocodoomData/\"+fname,\"r\"))\n",
    "    idImagesLink = {}\n",
    "    for images in f[\"images\"]:\n",
    "        idImagesLink[images[\"id\"]] = {\"file_name\":images[\"file_name\"], \"bbox\":[], \"category_id\":[]}\n",
    "\n",
    "    for annotation in f[\"annotations\"]:\n",
    "        idImagesLink[annotation[\"image_id\"]][\"bbox\"].append(annotation[\"bbox\"])\n",
    "        idImagesLink[annotation[\"image_id\"]][\"category_id\"].append(annotation[\"category_id\"])\n",
    "\n",
    "    for elem in idImagesLink:\n",
    "        dest = \"data/\"+idImagesLink[elem][\"file_name\"].replace(\"/rgb\",\"images\")\n",
    "        dest = re.sub(r'\\bmap\\d{1,2}', '', dest)\n",
    "        dest = dest.split('/')\n",
    "        dest[1], dest[2] = dest[2], dest[1]\n",
    "        dest = '/'.join(dest)\n",
    "        print(dest)\n",
    "        shutil.copy(src+idImagesLink[elem][\"file_name\"], dest)\n",
    "        with open(dest.replace(\"images\",\"labels\").replace(\".png\",\".txt\"),\"w\") as f:\n",
    "            # if len(idImagesLink[elem][\"bbox\"]) == 0:\n",
    "            #     f.write(str(0)+\"\\n\")\n",
    "            for i, bbox in enumerate(idImagesLink[elem][\"bbox\"]):\n",
    "                xmin = bbox[0]\n",
    "                ymin = bbox[1]\n",
    "                xmax = bbox[0]+bbox[2]\n",
    "                ymax = bbox[1]+bbox[3]\n",
    "                center_x = min(((xmin+xmax)//2)/width,1.0)\n",
    "                center_y = min(((ymin+ymax)//2)/height,1.0)\n",
    "                widthbb = min((xmax-xmin)/width,1)\n",
    "                heightbb = min((ymax-ymin)/height,1)\n",
    "                f.write(str(names.index(idImagesLink[elem][\"category_id\"][i]))+\" \"+str(center_x)+\" \"+str(center_y)+\" \"+str(widthbb)+\" \"+str(heightbb)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "count = defaultdict(int)\n",
    "for elem in idImagesLink.values():\n",
    "    for cat in elem[\"category_id\"]:\n",
    "        count[cat] += 1\n",
    "\n",
    "print(sorted(count.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Nous devons crée un fichier de config pour l'entrainement du model. Ce fichier contient les informations suivantes :\n",
    "- path: path vers les images d'entrainement\n",
    "- train: path vers le fichier contenant les images d'entrainement\n",
    "- valid: path vers le fichier contenant les images de validation\n",
    "- names: Ids et noms des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "data = dict(\n",
    "path= '/Users/engel/Documents/cocoDoom/datatest',\n",
    "train='images/run1',\n",
    "val= 'images/run1',\n",
    "\n",
    "names = \n",
    "{\n",
    "0 :  \"a\" ,\n",
    "1 :  \"b\" ,\n",
    "2 :  \"c\" ,\n",
    "3 :  \"d\" ,\n",
    "4 :  \"e\" ,\n",
    "5 :  \"f\" ,\n",
    "6 :  \"g\" ,\n",
    "7 :  \"h\" ,\n",
    "8 :  \"i\" ,\n",
    "9 :  \"j\" ,\n",
    "10 :  \"k\" ,\n",
    "11 :  \"l\" ,\n",
    "12 :  \"m\" ,\n",
    "13 :  \"o\" ,\n",
    "14 :  \"p\" ,\n",
    "15 :  \"q\" ,\n",
    "16 :  \"r\" ,\n",
    "17 :  \"s\" ,\n",
    "18 :  \"t\" ,\n",
    "19 :  \"u\" ,\n",
    "20 :  \"v\" ,\n",
    "21 :  \"w\" ,\n",
    "22 :  \"x\" ,\n",
    "23 :  \"z\" ,\n",
    "24 :  \"A\" ,\n",
    "25 :  \"B\" ,\n",
    "26 :  \"C\" ,\n",
    "27 :  \"D\" ,\n",
    "28 :  \"E\" ,\n",
    "29 :  \"F\" ,\n",
    "30 :  \"G\" ,\n",
    "31 :  \"H\" ,\n",
    "32 :  \"I\" ,\n",
    "33 :  \"J\" ,\n",
    "34 :  \"K\" ,\n",
    "35 :  \"L\" ,\n",
    "36 :  \"M\" ,\n",
    "37 :  \"O\" ,\n",
    "38 :  \"P\" ,\n",
    "39 :  \"Q\" ,\n",
    "40 :  \"R\" ,\n",
    "41 :  \"S\" ,\n",
    "42 :  \"T\" ,\n",
    "43 :  \"U\" ,\n",
    "44 :  \"V\" ,\n",
    "45 :  \"W\" ,\n",
    "46 :  \"X\" ,\n",
    "47 :  \"Z\" ,\n",
    "48 :  \"aa\" ,\n",
    "49 :  \"ba\" ,\n",
    "50 :  \"ca\" ,\n",
    "51 :  \"da\" ,\n",
    "52 :  \"ea\" ,\n",
    "53 :  \"fa\" ,\n",
    "54 :  \"ga\" ,\n",
    "55 :  \"ha\" ,\n",
    "56 :  \"ia\" ,\n",
    "57 :  \"ja\" ,\n",
    "58 :  \"ka\" ,\n",
    "59 :  \"la\" ,\n",
    "60 :  \"ma\" ,\n",
    "61 :  \"oa\" ,\n",
    "62 :  \"pa\" ,\n",
    "63 :  \"qa\" ,\n",
    "64 :  \"ra\" ,\n",
    "65 :  \"sa\" ,\n",
    "66 :  \"ta\" ,\n",
    "67 :  \"ua\" ,\n",
    "68 :  \"va\" ,\n",
    "69 :  \"wa\" ,\n",
    "70 :  \"xa\" ,\n",
    "71 :  \"za\" ,\n",
    "72 :  \"Aa\" ,\n",
    "73 :  \"Ba\" ,\n",
    "74 :  \"Ca\" ,\n",
    "75 :  \"Da\" ,\n",
    "76 :  \"Ea\" ,\n",
    "77 :  \"Fa\" ,\n",
    "78 :  \"Ga\" ,\n",
    "79 :  \"Ha\" ,\n",
    "80 :  \"Ia\" ,\n",
    "81 :  \"Ja\" ,\n",
    "82 :  \"Ka\" ,\n",
    "83 :  \"La\" ,\n",
    "84 :  \"Ma\" ,\n",
    "85 :  \"Oa\" ,\n",
    "86 :  \"Pa\" ,\n",
    "87 :  \"Qa\" ,\n",
    "88 :  \"Ra\" ,\n",
    "89 :  \"Sa\" ,\n",
    "90 :  \"Ta\" ,\n",
    "91 :  \"Ua\" ,\n",
    "92 :  \"Va\" ,\n",
    "93 :  \"Wa\" ,\n",
    "\n",
    "}\n",
    ")\n",
    "\n",
    "with open('data2.yml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle\n",
    "Nous pouvons désormais entrainer notre modèle sur notre dataset, nous utilison un modèle préetnainé sur le dataset 'COCO' pour la detection/segmentation et sur el dataset 'Imagenet' pour la classification. Nous avons entrainé le modèle sur 15 epochs car au delà on depasse les douzes heures maximus de run de kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb disabled\n",
    "!yolo detect train data='config.yaml' model='yolov8n.pt' epochs=15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédictions\n",
    "prediction sur des nouvelle donnée qu'on dump en json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "model = YOLO('results/runs/detect/train/weights/best.pt')  # initialize\n",
    "result = model.predict('data/images/run3/', conf=0.75)  # predict\n",
    "\n",
    "r = {}\n",
    "for i in result:\n",
    "    r[i.path] = (i.tojson())\n",
    "json.dump(r, open(\"resultconf0.75.json\",\"w\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Nous pouvons désormais évaluer les performances de notre modèle. Pour ce faire nous devons charger les données prédites ainsi que les données réeles afin de pouvoir les comparer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = json.load(open(\"resultconf0.75.json\",\"r\"))\n",
    "predictedData = {}\n",
    "for key, elem in Data.items():\n",
    "    print(key, elem)\n",
    "    predictedData[key] = json.loads(elem)\n",
    "\n",
    "f = json.load(open(\"cocodoomData/run-test.json\",\"r\"))\n",
    "groundTruthData = {}\n",
    "for images in f[\"images\"]:\n",
    "    groundTruthData[images[\"id\"]] = {\"file_name\":images[\"file_name\"], \"bbox\":[], \"category_id\":[]}\n",
    "\n",
    "for annotation in f[\"annotations\"]:\n",
    "    box = annotation[\"bbox\"]\n",
    "    box[2] += box[0]\n",
    "    box[3] += box[1]\n",
    "    groundTruthData[annotation[\"image_id\"]][\"bbox\"].append(box)\n",
    "    groundTruthData[annotation[\"image_id\"]][\"category_id\"].append(annotation[\"category_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIoU(predictedBox, trueBox):\n",
    "    x1 = max(predictedBox[0], trueBox[0])\n",
    "    y1 = max(predictedBox[1], trueBox[1])\n",
    "    x2 = min(predictedBox[2], trueBox[2])\n",
    "    y2 = min(predictedBox[3], trueBox[3])\n",
    "    intersection = max(0, x2-x1) * max(0, y2-y1)\n",
    "    union =( predictedBox[2]-predictedBox[0])*(predictedBox[3]-predictedBox[1]) + (trueBox[2]-trueBox[0])*(trueBox[3]-trueBox[1]) - intersection\n",
    "    # print(\"Intersection: \", intersection)\n",
    "    # print(\"Union: \", union)\n",
    "    return intersection/union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les données chargées, nous pouvons calculer les metrics suivants:\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "calcul des metrics, je vais probablment tout changer demain il y a des trucs préfait par yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = {}\n",
    "iouMoyenne = 0\n",
    "falsePrediction = 0\n",
    "totErrorNbBox = 0\n",
    "total = 0\n",
    "for predicetedkey, predictedvalue,truevalue in zip(predictedData.keys(), predictedData.values(),groundTruthData.values()):\n",
    "    iou[predicetedkey] = {'metric':[]}\n",
    "    ErrorNbBox = 0\n",
    "    if (len(predictedvalue) != len(truevalue[\"bbox\"])):\n",
    "        if (len(predictedvalue) > len(truevalue[\"bbox\"])):\n",
    "            ErrorNbBox = len(predictedvalue) - len(truevalue[\"bbox\"])\n",
    "        else:\n",
    "            ErrorNbBox = len(truevalue[\"bbox\"]) - len(predictedvalue)\n",
    "        totErrorNbBox += abs(ErrorNbBox)\n",
    "    for trueBox,trueid in zip(truevalue[\"bbox\"], truevalue[\"category_id\"]):\n",
    "        computedIoU = 0\n",
    "        predictedIdMaxIoU = 0\n",
    "        for predicted in predictedvalue:\n",
    "            predictedBox = [predicted['box'][\"x1\"], predicted['box'][\"y1\"], predicted['box'][\"x2\"], predicted['box'][\"y2\"]]\n",
    "            predictedId = predicted[\"class\"]+1\n",
    "            newIoU = computeIoU(predictedBox, trueBox)\n",
    "            if (computedIoU<newIoU):\n",
    "                computedIoU = newIoU\n",
    "                predictedIdMaxIoU = predictedId\n",
    "        if (len(predictedvalue) != 0):\n",
    "            falsePrediction += 1 if predictedIdMaxIoU!=trueid else 0\n",
    "            if predictedIdMaxIoU!=trueid:\n",
    "                print(\"Predicted ID: \", predictedId)\n",
    "                print(\"True ID: \", trueid)\n",
    "        total += 1\n",
    "        iouMoyenne += computedIoU\n",
    "        iou[predicetedkey]['metric'].append({ \"iou\": computedIoU, \"predictedId\": predictedId})\n",
    "        iou[predicetedkey]['errorNbBox'] = ErrorNbBox\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "print(\"IoU Moyenne: \", iouMoyenne/total)\n",
    "print(\"False Prediction: \", falsePrediction)\n",
    "print(\"Total: \", total)\n",
    "print(\"Total Error Nb Box: \", totErrorNbBox)\n",
    "iou['globalMetrics'] = {\"IoUMoyenne\": iouMoyenne/total, \"FalsePrediction\": falsePrediction, \"Total\": total, \"TotalErrorNbBox\": totErrorNbBox}\n",
    "json.dump(iou, open(\"iouconf0.75.json\",\"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelstr in ['resultLarge30', 'resultLarge', 'results']:  #dans l'ordre c'est val3, val4 et val 5\n",
    "    model = YOLO(f'{modelstr}/runs/detect/train/weights/best.pt')\n",
    "    res = model.val()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amélioration du modele\n",
    "\n",
    "Les même manipulation on été faites pour 2 autres modèles: le modèleyoloV8l entrainé sur 15 epochs et le modèles yolov8l entrainé sur 30 epochs (en réentrainant le modele a partir des poids de l'entrainement 15 epochs)\n",
    "\n",
    "Le modèle yolov8l (large) est un plus gros que yolov8n (nano). Ce qui le rends plus lent mais plus précis. (cfr https://docs.ultralytics.com/tasks/detect/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultat et interpretation\n",
    "\n",
    "\n",
    "<img src=\"resultLarge30\\runs\\detect\\train\\val_batch1_labels.jpg\" alt= “” width=\"value\" height=300>\n",
    "\n",
    "\n",
    "<img src=\"resultLarge30\\runs\\detect\\train\\val_batch1_pred.jpg\" alt= “” width=\"value\" height=300>\n",
    "\n",
    "## confusions matrix\n",
    "\n",
    "EN analisant la matrice de confusion resultant de l'entrainement du modele on peut observer que l'erreur la plus prédominante est la detection d'un objet comme etant un background. Cela veut dire que le modèle ne détecte rien dans l'image, cette erreur peut etre due a un manque d'entrainement ou a un threshold de confiance trop élevé. L'erreur inverse apparait aussi, c'est a dire que le modèle detecte un background comme etant un objet, cela est sans doute aussi du a un manque d'entrainement ou a un threshold de confiance trop bas. Plusieur classe sont aussi regulierement confonudes, probablment du a une ressemblance entre les objets de ces classes.\n",
    "\n",
    "Les matrices sont similaire pour les 3 modèles.\n",
    "\n",
    "<img src=\"resultLarge30\\runs\\detect\\train\\confusion_matrix_normalized.png\" alt= “” width=\"value\" height=600>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Nous avons vu que l'un des facteurs limitant la performance de notre modèle est le nombre d'epochs d'entrainement. En effet, nous avons entrainé notre modèle sur 15 epochs car au delà on depasse les douzes heures maximus de run de kaggle. Une manière de contourner ce modèle et de réentrainer notre modèle sur 15 epochs en utilisant les poids du modèle précédent. On pourrais ainsi continuer à entrainer notre modèle sur 15 epochs jusqu'à ce que les performances ne s'améliorent plus.\n",
    "Un autre facteur important est la taille du modèle. En effet, nous avons commencé avec yolo nano avant de passer a yolo large. Nous avons pu constater que le modèle large est plus performant que le modèle nano. Cependant, le modèle large est plus long à entrainer et plus long à prédire. Il serait donc intéressant de trouver un compromis entre la taille du modèle et ses performances.\n",
    "\n",
    "\n",
    "amelioration : mettre vrai nom de classe"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
